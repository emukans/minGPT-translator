{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "optimized with valid Proper packages minGPT EN-LV translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emukans/minGPT-translator/blob/master/optimized_with_valid_Proper_packages_minGPT_EN_LV_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lEj5q8Mlgl6"
      },
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "id": "5lEj5q8Mlgl6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwaEm_7F6Yqp",
        "outputId": "e8b06954-3651-4d6c-fb00-a24be0b4b376"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "SwaEm_7F6Yqp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug 22 16:36:56 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "varying-atlas"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import os\n",
        "import urllib\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from collections import OrderedDict, Counter\n",
        "\n",
        "%matplotlib inline"
      ],
      "id": "varying-atlas",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53ONmxoOw6-K"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(1)"
      ],
      "id": "53ONmxoOw6-K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALC2-FQxnZ1l",
        "outputId": "4b41c04c-bcdb-4308-f3f3-6244b720f545"
      },
      "source": [
        "!pip install sacrebleu"
      ],
      "id": "ALC2-FQxnZ1l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▋                            | 10 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 20 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 30 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 51 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 61 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 71 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 81 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 90 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.19.5)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.4 portalocker-2.3.1 sacrebleu-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A8WZUBGiyv7",
        "outputId": "69e35be1-5db6-435e-801b-96f4105817ea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "7A8WZUBGiyv7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex11yfCVaueI"
      },
      "source": [
        "#Text preprocessing"
      ],
      "id": "Ex11yfCVaueI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "historic-thermal"
      },
      "source": [
        "# Read Hemingway texts from URL. There are Hemingway's \"A Farewell to arms\"\n",
        "text_input = urllib.request.urlopen('http://www.ltn.lv/~guntis/translation_dataset/dataset_en_small.txt').read().decode(\"utf-8\", \"ignore\")\n",
        "text_output = urllib.request.urlopen('http://www.ltn.lv/~guntis/translation_dataset/dataset_lv_small.txt').read().decode(\"utf-8-sig\", \"ignore\")\n",
        "\n",
        "# text_input = ''.join(open('/content/drive/MyDrive/tilde en-lv data/train.lv').readlines()[:10000])\n",
        "# text_output = ''.join(open('/content/drive/MyDrive/tilde en-lv data/train.en').readlines()[:10000])\n",
        "\n",
        "with open('en.txt', 'w') as f:\n",
        "    f.write(text_input)\n",
        "\n",
        "with open('lv.txt', 'w') as f:\n",
        "    f.write(text_output)"
      ],
      "id": "historic-thermal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzgEuTRkV4kK",
        "outputId": "582cb3cd-5d2a-48b5-eed1-625cd3e0838c"
      },
      "source": [
        "!git clone https://github.com/moses-smt/mosesdecoder.git"
      ],
      "id": "hzgEuTRkV4kK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mosesdecoder'...\n",
            "remote: Enumerating objects: 148070, done.\u001b[K\n",
            "remote: Counting objects: 100% (498/498), done.\u001b[K\n",
            "remote: Compressing objects: 100% (206/206), done.\u001b[K\n",
            "remote: Total 148070 (delta 315), reused 433 (delta 289), pack-reused 147572\u001b[K\n",
            "Receiving objects: 100% (148070/148070), 129.86 MiB | 19.43 MiB/s, done.\n",
            "Resolving deltas: 100% (114341/114341), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_qVpRZMGJDl",
        "outputId": "fadf3292-17ce-458c-9cc7-335511140ea6"
      },
      "source": [
        "# Normalize and tokenize texts\n",
        "\n",
        "!cat en.txt | mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l en \\\n",
        "  | mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l en > en.tok.txt\n",
        "\n",
        "!cat lv.txt | mosesdecoder/scripts/tokenizer/normalize-punctuation.perl -l lv \\\n",
        "  | mosesdecoder/scripts/tokenizer/tokenizer.perl -a -l lv > lv.tok.txt"
      ],
      "id": "Z_qVpRZMGJDl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 1\n",
            "Tokenizer Version 1.1\n",
            "Language: lv\n",
            "Number of threads: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlDFOYvOIb2c"
      },
      "source": [
        "!mosesdecoder/scripts/recaser/train-truecaser.perl -corpus en.tok.txt -model tc.en\n",
        "!mosesdecoder/scripts/recaser/train-truecaser.perl -corpus lv.tok.txt -model tc.lv\n",
        "\n",
        "!mosesdecoder/scripts/recaser/truecase.perl -model tc.en < en.tok.txt > en.tc.txt\n",
        "!mosesdecoder/scripts/recaser/truecase.perl -model tc.lv < lv.tok.txt > lv.tc.txt"
      ],
      "id": "YlDFOYvOIb2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTfvBWhIaHTh",
        "outputId": "89767a27-d7a1-4635-bad7-b0aeba8cb907"
      },
      "source": [
        "!pip install subword-nmt"
      ],
      "id": "bTfvBWhIaHTh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.7-py2.py3-none-any.whl (26 kB)\n",
            "Installing collected packages: subword-nmt\n",
            "Successfully installed subword-nmt-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiMbyzugbFZd"
      },
      "source": [
        "!subword-nmt learn-joint-bpe-and-vocab --input en.tc.txt lv.tc.txt -s 3000 -o tokens.lven --write-vocabulary token_freq.en.txt token_freq.lv.txt\n",
        "# !subword-nmt learn-joint-bpe-and-vocab --input en.tc.txt -s 5000 -o tokens.en.txt --write-vocabulary token_freq.en.txt\n",
        "# !subword-nmt learn-joint-bpe-and-vocab --input lv.tc.txt -s 5000 -o tokens.lv.txt --write-vocabulary token_freq.lv.txt"
      ],
      "id": "aiMbyzugbFZd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30_qUBkgSNG1"
      },
      "source": [
        "def build_vocab(freq_file):\n",
        "    vocab = Counter(['<pad>', '<eos>', '<sep>', '<unk>'])\n",
        "    with open(freq_file, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            token, num_occurs = line.split()\n",
        "            vocab[token] += int(num_occurs)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "en_vocab = build_vocab('token_freq.en.txt')\n",
        "lv_vocab = build_vocab('token_freq.lv.txt')\n",
        "\n",
        "en_vocab = build_vocab('token_freq.en.txt')\n",
        "lv_vocab = build_vocab('token_freq.lv.txt')\n",
        "\n",
        "\n",
        "joint_vocab = Counter(en_vocab)\n",
        "joint_vocab.update(lv_vocab)\n",
        "\n",
        "with open('vocab.lven', 'w') as f:\n",
        "    for i, token in enumerate(joint_vocab):\n",
        "        f.write(f\"{token} {joint_vocab[token]} \\n\")"
      ],
      "id": "30_qUBkgSNG1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtzY4LjYbSiJ"
      },
      "source": [
        "!subword-nmt apply-bpe -c tokens.lven --vocabulary vocab.lven --vocabulary-threshold 10 < en.tc.txt > en.BPE.txt\n",
        "!subword-nmt apply-bpe -c tokens.lven --vocabulary vocab.lven --vocabulary-threshold 10 < lv.tc.txt > lv.BPE.txt"
      ],
      "id": "CtzY4LjYbSiJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcc2v_fHe81_"
      },
      "source": [
        "with open('lv.BPE.txt', 'r') as f:\n",
        "    text_input = f.read()\n",
        "\n",
        "with open('en.BPE.txt', 'r') as f:\n",
        "    text_output = f.read()"
      ],
      "id": "wcc2v_fHe81_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAHdOeNhYJ8v"
      },
      "source": [
        "#MinGPT"
      ],
      "id": "PAHdOeNhYJ8v"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcFt-zMBfqDt"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "def calculate_attention_token(attention, top_k, model):\n",
        "    logits = model.head(attention)\n",
        "    logits = logits[:, -1, :]\n",
        "    logits = top_k_logits(logits, top_k)\n",
        "\n",
        "    probs = F.softmax(logits)\n",
        "\n",
        "    _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "    ix = torch.multinomial(probs, num_samples=top_k)\n",
        "\n",
        "    return ix[0]\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None, output_attention=False):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
        "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
        "    of block_size, unlike an RNN that has an infinite context window.\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    model.eval()\n",
        "    attention_state = [[] for _ in model.blocks]\n",
        "\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits, _ = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "\n",
        "        if output_attention:\n",
        "            b, t = x.size()\n",
        "\n",
        "            for block_id in range(len(model.blocks)):\n",
        "                att = model.blocks[block_id].attn.att\n",
        "                attention_state[block_id].append(att)\n",
        "\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "    if output_attention:\n",
        "        return x, attention_state\n",
        "\n",
        "    return x\n"
      ],
      "id": "OcFt-zMBfqDt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZGEd6UCfeou"
      },
      "source": [
        "\"\"\"\n",
        "GPT model:\n",
        "- the initial stem consists of a combination of token encoding and a positional encoding\n",
        "- the meat of it is a uniform sequence of Transformer blocks\n",
        "    - each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n",
        "    - all blocks feed into a central residual pathway similar to resnets\n",
        "- the final decoder is a linear projection into a vanilla Softmax classifier\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GPTConfig:\n",
        "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
        "    embd_pdrop = 0.1\n",
        "    resid_pdrop = 0.1\n",
        "    attn_pdrop = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class GPT1Config(GPTConfig):\n",
        "    \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
        "    n_layer = 12\n",
        "    n_head = 12\n",
        "    n_embd = 768\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.att = None\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "\n",
        "        self.att = att\n",
        "\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        # transformer\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"\n",
        "        This long function is unfortunately doing something very simple and is being very defensive:\n",
        "        We are separating out all parameters of the model into two buckets: those that will experience\n",
        "        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).\n",
        "        We are then returning the PyTorch optimizer object.\n",
        "        \"\"\"\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # special case the position embedding parameter in the root GPT module as not decayed\n",
        "        no_decay.add('pos_emb')\n",
        "\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "\n",
        "        # forward the GPT model\n",
        "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
        "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0)\n",
        "\n",
        "        return logits, loss\n"
      ],
      "id": "cZGEd6UCfeou",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_c86fg-UKSK"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "def plot_loss(train_loss_list, test_loss_list, valid_loss_list, valid_bleu_list):\n",
        "    epochs = range(len(test_loss_list))\n",
        "    fig, axs = plt.subplots(nrows=4, ncols=1, figsize=(30, 20))\n",
        "    axs[0].plot(epochs, train_loss_list)\n",
        "    axs[0].set_title('Train loss')\n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "\n",
        "    axs[1].plot(epochs, test_loss_list)\n",
        "    axs[1].set_title('Test loss')\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "\n",
        "    axs[2].plot(epochs, valid_loss_list)\n",
        "    axs[2].set_title('Validation loss')\n",
        "    axs[2].set_xlabel('Epochs')\n",
        "    axs[2].set_ylabel('Loss')\n",
        "\n",
        "    axs[3].plot(epochs, valid_bleu_list)\n",
        "    axs[3].set_title('Validation BLEU')\n",
        "    axs[3].set_xlabel('Epochs')\n",
        "    axs[3].set_ylabel('BLEU')\n",
        "\n",
        "    plt.show()"
      ],
      "id": "w_c86fg-UKSK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s5SO83OXVK6"
      },
      "source": [
        "\"\"\"\n",
        "Simple training loop; Boilerplate that could apply to any arbitrary neural network,\n",
        "so nothing in this file really has anything to do with GPT specifically.\n",
        "\"\"\"\n",
        "\n",
        "import sacrebleu\n",
        "import math\n",
        "import logging\n",
        "from random import choice\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def clean_tokens(sentence):\n",
        "    return sentence.replace('@@ ', '').replace(' @', '').replace('@ ', '')\n",
        "\n",
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 3e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    grad_norm_clip = 1.0\n",
        "    weight_decay = 0.1 # only applied on matmul weights\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False\n",
        "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "    # checkpoint settings\n",
        "    ckpt_path = None\n",
        "    num_workers = 0 # for DataLoader\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, train_dataset, test_dataset, valid_dataset, config):\n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.valid_dataset = valid_dataset\n",
        "        self.config = config\n",
        "\n",
        "        # take over whatever gpus are on the system\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.cuda.current_device()\n",
        "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
        "\n",
        "    def save_checkpoint(self, postfix=''):\n",
        "        # DataParallel wrappers keep raw model object in .module attribute\n",
        "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
        "        checkpoint_path = self.config.ckpt_path + postfix + '.pt'\n",
        "        logger.info(\"saving %s\", checkpoint_path)\n",
        "        torch.save(raw_model.state_dict(), checkpoint_path)\n",
        "\n",
        "    def train(self):\n",
        "        model, config = self.model, self.config\n",
        "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
        "        optimizer = raw_model.configure_optimizers(config)\n",
        "\n",
        "        def run_epoch(split):\n",
        "            is_train = split == 'train'\n",
        "            model.train(is_train)\n",
        "            data = self.train_dataset\n",
        "            if split == 'test':\n",
        "                data = self.test_dataset\n",
        "            elif split == 'valid':\n",
        "                data = self.valid_dataset\n",
        "                model.eval()\n",
        "            loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
        "                                batch_size=config.batch_size,\n",
        "                                num_workers=config.num_workers)\n",
        "\n",
        "            losses = []\n",
        "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
        "            logits_total = None\n",
        "            x_total = None\n",
        "            y_total = None\n",
        "            for it, (x, y) in pbar:\n",
        "\n",
        "                # place data on the correct device\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "\n",
        "                # forward the model\n",
        "                with torch.set_grad_enabled(is_train):\n",
        "                    logits, loss = model(x, y)\n",
        "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
        "                    losses.append(loss.item())\n",
        "                    if split == 'valid':\n",
        "                        if logits_total is None:\n",
        "                            logits_total = logits\n",
        "                            x_total = x\n",
        "                            y_total = y\n",
        "                        else:\n",
        "                            logits_total = torch.cat((logits_total, logits), dim=0)\n",
        "                            x_total = torch.cat((x_total, x), dim=0)\n",
        "                            y_total = torch.cat((y_total, y), dim=0)\n",
        "                        \n",
        "\n",
        "                if is_train:\n",
        "                    # backprop and update the parameters\n",
        "                    model.zero_grad()\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # decay the learning rate based on our progress\n",
        "                    if config.lr_decay:\n",
        "                        # self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "                        self.tokens += (y > 0).sum() # number of tokens processed this step (i.e. label is not -100) !!!in our case is not 0\n",
        "                        if self.tokens < config.warmup_tokens:\n",
        "                            # linear warmup\n",
        "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
        "                        else:\n",
        "                            # cosine learning rate decay\n",
        "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "                        lr = config.learning_rate * lr_mult\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                    else:\n",
        "                        lr = config.learning_rate\n",
        "\n",
        "                    # report progress\n",
        "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. mean loss: {float(np.mean(losses)):.5f}. lr {lr:e}\")\n",
        "\n",
        "            if split == 'train':\n",
        "                train_loss = float(np.mean(losses))\n",
        "                print(f\"train loss: {train_loss}\")\n",
        "\n",
        "                return train_loss\n",
        "\n",
        "            if split == 'test':\n",
        "                test_loss = float(np.mean(losses))\n",
        "                print(f\"test loss: {test_loss}\")\n",
        "                return test_loss\n",
        "\n",
        "            if split == 'valid':\n",
        "                test_loss = float(np.mean(losses))\n",
        "                print(f\"valid loss: {test_loss}\")\n",
        "\n",
        "                eval_results = []\n",
        "                translation_results = []\n",
        "                context_list = []\n",
        "\n",
        "                for idx in range(len(logits_total)):\n",
        "                    intent = (x_total[idx] == valid_dataset.tokenizer_input.encode(['<sep>'])[0]).nonzero(as_tuple=True)[0][0]\n",
        "\n",
        "                    probs = F.softmax(logits_total[idx], dim=-1)\n",
        "                    # sample from the distribution or take the most likely\n",
        "                    _, predicted = torch.topk(probs, k=1, dim=-1)\n",
        "                    context = clean_tokens(data.tokenizer_input.decode(x_total[idx][:intent - 1], True))\n",
        "                    completion = clean_tokens(data.tokenizer_output.decode(predicted[intent:], True))\n",
        "                    real = clean_tokens(data.tokenizer_output.decode(y_total[idx][intent:], True))\n",
        "\n",
        "                    context_list.append(context)\n",
        "                    translation_results.append(completion)\n",
        "                    eval_results.append(real)\n",
        "                \n",
        "                with open('valid.txt', 'w') as f:\n",
        "                    f.write(\"\\n\".join(translation_results))\n",
        "\n",
        "                with open('eval.txt', 'w') as f:\n",
        "                    f.write(\"\\n\".join(eval_results))\n",
        "\n",
        "                with open('context.txt', 'w') as f:\n",
        "                    f.write(\"\\n\".join(context_list))\n",
        "\n",
        "\n",
        "                !cat valid.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > valid.detok.txt\n",
        "                !cat eval.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > eval.detok.txt\n",
        "                !cat context.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > context.detok.txt\n",
        "\n",
        "                with open('eval.detok.txt', 'r') as f:\n",
        "                    eval_results = [l.strip() for l in f.readlines()]\n",
        "                with open('valid.detok.txt', 'r') as f:\n",
        "                    translation_results = [l.strip() for l in f.readlines()]\n",
        "                with open('context.detok.txt', 'r') as f:\n",
        "                    context_list = [l.strip() for l in f.readlines()]\n",
        "\n",
        "                valid_sentences = ['the driver wore a cap and his face was thin and very tanned.',\n",
        "                                   'outside it was getting dark.',\n",
        "                                   'the two girls were asleep.',\n",
        "                                   'I would like to have had the uniform off although I did not care much about the outward forms.',\n",
        "                                   'I watched the flashes on San Gabriele.',\n",
        "                                   'I asked.',\n",
        "                                   '\"no.']\n",
        "\n",
        "                idx_list = [i for i, sentence in enumerate(eval_results) if sentence in valid_sentences]\n",
        "\n",
        "                for idx in idx_list:\n",
        "                    print(f'Input:            {context_list[idx]}')\n",
        "                    print(f'Predicted output: {translation_results[idx]}')\n",
        "                    print(f'Real output:      {eval_results[idx]}')\n",
        "                    print('--------------------------------------------------')\n",
        "\n",
        "                refs = [eval_results]\n",
        "                sys = translation_results\n",
        "                bleu = sacrebleu.corpus_bleu(sys, refs)\n",
        "                print(f'BLEU: {bleu.score}')\n",
        "                print('##############################################################')\n",
        "\n",
        "                return test_loss, bleu.score\n",
        "\n",
        "        train_loss_list = []\n",
        "        test_loss_list = []\n",
        "        valid_loss_list = []\n",
        "        valid_bleu_list = []\n",
        "        best_loss = float('inf')\n",
        "        self.tokens = 0 # counter used for learning rate decay\n",
        "        for epoch in range(config.max_epochs):\n",
        "\n",
        "            train_loss = run_epoch('train')\n",
        "            train_loss_list.append(train_loss)\n",
        "            if self.test_dataset is not None:\n",
        "                test_loss = run_epoch('test')\n",
        "                test_loss_list.append(test_loss)\n",
        "\n",
        "            if self.valid_dataset is not None:\n",
        "                valid_loss, bleu_score = run_epoch('valid')\n",
        "                valid_loss_list.append(valid_loss)\n",
        "                valid_bleu_list.append(bleu_score)\n",
        "\n",
        "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
        "            good_model = self.test_dataset is None or valid_loss < best_loss\n",
        "            if self.config.ckpt_path is not None and good_model:\n",
        "                best_loss = valid_loss\n",
        "                self.save_checkpoint(\"_best\")\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                self.save_checkpoint(f\"_{epoch}\")\n",
        "                # plot_loss(train_loss_list, test_loss_list, valid_loss_list, valid_bleu_list)\n",
        "\n",
        "            self.save_checkpoint(\"_last\")\n",
        "\n",
        "        return train_loss_list, test_loss_list, valid_loss_list, valid_bleu_list\n"
      ],
      "id": "0s5SO83OXVK6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKCnnx4-XTg1"
      },
      "source": [
        "#Training"
      ],
      "id": "jKCnnx4-XTg1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds0Rr1hrbRDq"
      },
      "source": [
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, data, vocab_size, vocab):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.stoi = {ch: i for i, ch in enumerate(self.vocab)}\n",
        "        self.itos = {i: ch for i, ch in enumerate(self.vocab)}\n",
        "    \n",
        "    def tokenize(self, data, block_size):\n",
        "        tokenized_text = data.split()\n",
        "        # Filter empty strings\n",
        "        tokenized_text = [x for x in tokenized_text if x]\n",
        "        result = []\n",
        "        for tokenized in tokenized_text:\n",
        "            # In case other single # found, replace them with <unk> special token, marking the element as unknown\n",
        "            if tokenized in self.vocab:\n",
        "                result.append(tokenized)\n",
        "            else:\n",
        "                result.append('<unk>')\n",
        "\n",
        "        # in case the sentence is longer, than block_size, we trim the sentence\n",
        "        return result[:block_size]\n",
        "    \n",
        "    def encode(self, data):\n",
        "        return [self.stoi[s] for s in data]\n",
        "    \n",
        "    def decode(self, data, clean_paddings=False):\n",
        "        token_list = [self.itos[int(i)] for i in data]\n",
        "        text = ' '.join(token_list)\n",
        "\n",
        "        if not clean_paddings:\n",
        "            return text\n",
        "\n",
        "        eos_index = (data == self.encode(['<eos>'])[0]).nonzero(as_tuple=True)[0]\n",
        "        if len(eos_index):\n",
        "            eos_index = eos_index[0]\n",
        "            text = ' '.join(token_list[:eos_index])\n",
        "\n",
        "        return text.replace('<pad>', '').replace('  ', '')"
      ],
      "id": "Ds0Rr1hrbRDq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advised-shelter"
      },
      "source": [
        "# vocab_size = 10000\n",
        "\n",
        "# vocab_input = None\n",
        "# if os.path.exists('vocab_input.pkl'):\n",
        "#     with open('vocab_input.pkl', 'rb') as f:\n",
        "#         vocab_input = pickle.load(f)\n",
        "        \n",
        "# vocab_output = None\n",
        "# if os.path.exists('vocab_output.pkl'):\n",
        "#     with open('vocab_output.pkl', 'rb') as f:\n",
        "#         vocab_output = pickle.load(f)\n",
        "\n",
        "# building vocabluary can take some time. ~5 minutes for 10_000 tokens for each tokenizer. \n",
        "tokenizer_input = Tokenizer(text_input, len(joint_vocab), list(joint_vocab))\n",
        "tokenizer_output = Tokenizer(text_output, len(joint_vocab), list(joint_vocab))"
      ],
      "id": "advised-shelter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d8f6ac2-0118-4d55-99c0-a478bfa6fe27"
      },
      "source": [
        "# with open('vocab_input.pkl', 'wb') as f:\n",
        "#     pickle.dump(tokenizer_input.vocab, f)\n",
        "\n",
        "# with open('vocab_output.pkl', 'wb') as f:\n",
        "#     pickle.dump(tokenizer_output.vocab, f)"
      ],
      "id": "7d8f6ac2-0118-4d55-99c0-a478bfa6fe27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emotional-metadata"
      },
      "source": [
        "# Shuffle texts by lines\n",
        "texts = list(zip(text_output.splitlines(), text_input.splitlines()))\n",
        "random.shuffle(texts)\n",
        "\n",
        "text_output, text_input = zip(*texts)"
      ],
      "id": "emotional-metadata",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "floral-ridge"
      },
      "source": [
        "# Split texts into train, test and validation datasets\n",
        "train_dataset_size = round(0.85 * len(text_output))\n",
        "valid_dataset_size = round(0.15 * len(text_output))\n",
        "\n",
        "train_input = text_input[:train_dataset_size]\n",
        "valid_input = text_input[-valid_dataset_size:]\n",
        "\n",
        "train_output = text_output[:train_dataset_size]\n",
        "valid_output = text_output[-valid_dataset_size:]\n"
      ],
      "id": "floral-ridge",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZUwkrJeb0p4"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class WordDataset(Dataset):\n",
        "\n",
        "    def __init__(self, output_text, input_text, tokenizer_output, tokenizer_input, block_size):\n",
        "        self.tokenizer_output = tokenizer_output\n",
        "        self.tokenizer_input = tokenizer_input\n",
        "\n",
        "        self.block_size = block_size * 2 + 2\n",
        "        self.output_text = [tokenizer_output.tokenize(t, block_size) for t in output_text]\n",
        "        self.input_text = [tokenizer_input.tokenize(t, block_size) for t in input_text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.output_text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        The idea is to get the input sentence\n",
        "        and translate it to output sentence (sentences could be on any language).\n",
        "\n",
        "        In the init method we already split a sentence into tokens and filled with spaces,\n",
        "        to have an equal sentence size. In this method we just encode the tokens to\n",
        "        ids (a list of numbers), and we're trying to map ids sequences\n",
        "        \"\"\"\n",
        "\n",
        "        tokenized_input_text = self.tokenizer_input.encode(self.input_text[idx])\n",
        "        tokenized_output_text = self.tokenizer_output.encode(self.output_text[idx])\n",
        "\n",
        "        dix = tokenized_input_text + self.tokenizer_output.encode(['<sep>']) + tokenized_output_text + self.tokenizer_output.encode(['<eos>'])\n",
        "        if len(dix) < self.block_size:\n",
        "            dix += self.tokenizer_output.encode(['<pad>']) * (self.block_size - len(dix))\n",
        "\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        x[len(tokenized_input_text) + len(tokenized_output_text):] = self.tokenizer_output.encode(['<pad>'])[0]\n",
        "        y[:len(tokenized_input_text)] = self.tokenizer_output.encode(['<pad>'])[0]\n",
        "\n",
        "        return x, y"
      ],
      "id": "oZUwkrJeb0p4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fitted-resident"
      },
      "source": [
        "block_size = 100  # the estimate how long lines the text could be (token count)\n",
        "\n",
        "train_dataset = WordDataset(train_output, train_input, tokenizer_output, tokenizer_input, block_size)\n",
        "valid_dataset = WordDataset(valid_output, valid_input, tokenizer_output, tokenizer_input, block_size)"
      ],
      "id": "fitted-resident",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruled-astronomy"
      },
      "source": [
        "# number_of_heads = 8\n",
        "# number_of_layers = 6\n",
        "number_of_heads = 4\n",
        "number_of_layers = 2\n",
        "\n",
        "# from mingpt.model import GPT, GPTConfig\n",
        "embd_pdrop = 0.1\n",
        "resid_pdrop = 0.1\n",
        "attn_pdrop = 0.1\n",
        "\n",
        "max_vocab = max(tokenizer_input.vocab_size, tokenizer_output.vocab_size)\n",
        "\n",
        "mconf = GPTConfig(max_vocab, train_dataset.block_size,\n",
        "                  n_layer=number_of_layers, n_head=number_of_heads, n_embd=512,\n",
        "                  embd_pdrop=embd_pdrop, resid_pdrop=resid_pdrop, attn_pdrop=attn_pdrop)\n",
        "model = GPT(mconf)"
      ],
      "id": "ruled-astronomy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUgb5MxODXBc"
      },
      "source": [
        "# from mingpt.trainer import Trainer, TrainerConfig\n",
        "\n",
        "tokens_per_epoch = len(train_dataset) * block_size\n",
        "train_epochs = 100\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "tconf = TrainerConfig(max_epochs=train_epochs, batch_size=64, learning_rate=3e-5,\n",
        "                      lr_decay=True, warmup_tokens=tokens_per_epoch, final_tokens=train_epochs*tokens_per_epoch,\n",
        "                      ckpt_path='drive/MyDrive/minGPT LV-EN/translator_model',\n",
        "                      num_workers=1, weight_decay=0.01, betas=(0.9, 0.95))\n",
        "trainer = Trainer(model, train_dataset, None, valid_dataset, tconf)"
      ],
      "id": "bUgb5MxODXBc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPH3nCe9JB63",
        "outputId": "bf5bbadb-4fec-4d73-f0d3-77baf2ad4a4a"
      },
      "source": [
        "param_count = sum([param.nelement() for param in model.parameters()])\n",
        "\n",
        "print(f'Parameters count: {param_count}')"
      ],
      "id": "JPH3nCe9JB63",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters count: 9562112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VG17yBsSf3uc",
        "outputId": "42f21b3b-1fd0-427f-bc98-05de3e8ee679"
      },
      "source": [
        "train_loss_list, valid_loss_list, valid_bleu_list = trainer.train()"
      ],
      "id": "VG17yBsSf3uc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1 iter 234: train loss 7.24386. mean loss: 7.63959. lr 4.330574e-06: 100%|██████████| 235/235 [00:53<00:00,  4.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 7.639588315436181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 6.830620947338286\n",
            "Warning: No built-in rules for language lv.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lv\n",
            "Warning: No built-in rules for language lv.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lv\n",
            "Warning: No built-in rules for language lv.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lv\n",
            "Input:            \"nē\n",
            "Predicted output: \"\" \"\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: \"..\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: \"..\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            vaicāju\n",
            "Predicted output: \n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            virs Sangabrielas ik pa brīdim nozibsnīja\n",
            "Predicted output: \n",
            "Real output:      I watched the flashes on San Gabriele.\n",
            "--------------------------------------------------\n",
            "Input:            nē\n",
            "Predicted output: \"\" \"\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: \"..\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"\" \"\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            labprāt būtu aizmetis šo uniformu, lai gan es daudz nebēdāju par ārieni\n",
            "Predicted output: \n",
            "Real output:      I would like to have had the uniform off although I did not care much about the outward forms.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"\" \"\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"\" \"\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"\" \"\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            metās jau tumsa\n",
            "Predicted output: \"\" \".\" \"\n",
            "Real output:      outside it was getting dark.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"\" \"\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: \"..\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            abas meitenes bija iemigušas\n",
            "Predicted output: \n",
            "Real output:      the two girls were asleep.\n",
            "--------------------------------------------------\n",
            "Input:            šoferim galvā bija cepure, un viņa seja bija kalsna un stipri iedegusi\n",
            "Predicted output: \n",
            "Real output:      the driver wore a cap and his face was thin and very tanned.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"\" \"\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "BLEU: 0.04435295084199963\n",
            "##############################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 2 iter 234: train loss 5.88744. mean loss: 6.32486. lr 8.661148e-06: 100%|██████████| 235/235 [00:53<00:00,  4.38it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 6.324862717567606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 5.75109170732044\n",
            "Warning: No built-in rules for language lv.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lv\n",
            "Warning: No built-in rules for language lv.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lv\n",
            "Warning: No built-in rules for language lv.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lv\n",
            "Input:            \"nē\n",
            "Predicted output: \"..\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"..\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            virs Sangabrielas ik pa brīdim nozibsnīja\n",
            "Predicted output: the...........\n",
            "Real output:      I watched the flashes on San Gabriele.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: ...\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            šoferim galvā bija cepure, un viņa seja bija kalsna un stipri iedegusi\n",
            "Predicted output: the the the the the the the the the the the the. the the.\n",
            "Real output:      the driver wore a cap and his face was thin and very tanned.\n",
            "--------------------------------------------------\n",
            "Input:            vaicāju\n",
            "Predicted output: ...\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"..\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            labprāt būtu aizmetis šo uniformu, lai gan es daudz nebēdāju par ārieni\n",
            "Predicted output: the the.. the....................\n",
            "Real output:      I would like to have had the uniform off although I did not care much about the outward forms.\n",
            "--------------------------------------------------\n",
            "Input:            abas meitenes bija iemigušas\n",
            "Predicted output: ......\n",
            "Real output:      the two girls were asleep.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: ...\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: ...\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"..\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"..\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"..\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: ...\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            metās jau tumsa\n",
            "Predicted output: ......\n",
            "Real output:      outside it was getting dark.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"..\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            nē\n",
            "Predicted output: ...\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "BLEU: 0.11211305038869167\n",
            "##############################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 3 iter 234: train loss 4.55245. mean loss: 5.49018. lr 1.299172e-05: 100%|██████████| 235/235 [00:53<00:00,  4.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 5.490181561733814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid loss: 5.177825167065575\n",
            "Warning: No built-in rules for language lv.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lv\n",
            "Warning: No built-in rules for language lv.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lv\n",
            "Warning: No built-in rules for language lv.\n",
            "Detokenizer Version $Revision: 4134 $\n",
            "Language: lv\n",
            "Input:            \"nē\n",
            "Predicted output: \"I.\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"I.\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: I..\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"I.\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            abas meitenes bija iemigušas\n",
            "Predicted output: I.....\n",
            "Real output:      the two girls were asleep.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: I..\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"I.\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            labprāt būtu aizmetis šo uniformu, lai gan es daudz nebēdāju par ārieni\n",
            "Predicted output: I the the the the the...................\n",
            "Real output:      I would like to have had the uniform off although I did not care much about the outward forms.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"I.\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            šoferim galvā bija cepure, un viņa seja bija kalsna un stipri iedegusi\n",
            "Predicted output: I the the the the and the the the the the the....\n",
            "Real output:      the driver wore a cap and his face was thin and very tanned.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: I..\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            vaicāju\n",
            "Predicted output: I..\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"I.\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            metās jau tumsa\n",
            "Predicted output: I.....\n",
            "Real output:      outside it was getting dark.\n",
            "--------------------------------------------------\n",
            "Input:            virs Sangabrielas ik pa brīdim nozibsnīja\n",
            "Predicted output: I the the.... the....\n",
            "Real output:      I watched the flashes on San Gabriele.\n",
            "--------------------------------------------------\n",
            "Input:            nē\n",
            "Predicted output: I I.\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            \"nē\n",
            "Predicted output: \"I.\n",
            "Real output:      \"no.\n",
            "--------------------------------------------------\n",
            "Input:            es jautāju\n",
            "Predicted output: I..\n",
            "Real output:      I asked.\n",
            "--------------------------------------------------\n",
            "BLEU: 2.0245408530633946\n",
            "##############################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 4 iter 234: train loss 4.53806. mean loss: 5.02580. lr 1.732230e-05: 100%|██████████| 235/235 [00:53<00:00,  4.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 5.025804217318271\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-8ce267af4031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_bleu_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-c5d951ff1b65>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m                 \u001b[0mvalid_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mvalid_bleu_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c5d951ff1b65>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m    104\u001b[0m                             \u001b[0my_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                             \u001b[0mlogits_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                             \u001b[0mx_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                             \u001b[0my_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.05 GiB (GPU 0; 11.17 GiB total capacity; 3.21 GiB already allocated; 1.40 GiB free; 9.32 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXqcI6JZ4wcG"
      },
      "source": [
        "plot_loss(train_loss_list, test_loss_list, valid_loss_list, valid_bleu_list)"
      ],
      "id": "eXqcI6JZ4wcG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp9SXgMdRdKV"
      },
      "source": [
        "print(f\"Max BLEU: {max(valid_bleu_list)}\")"
      ],
      "id": "mp9SXgMdRdKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Og-0W-MtKoM"
      },
      "source": [
        "#Evaluate"
      ],
      "id": "1Og-0W-MtKoM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d029ed9c-9170-435a-805f-416774999287"
      },
      "source": [
        "checkpoint = torch.load('drive/MyDrive/minGPT LV-EN/translator_model_best.pt')\n",
        "model.load_state_dict(checkpoint)"
      ],
      "id": "d029ed9c-9170-435a-805f-416774999287",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afC6uEVGXIPL"
      },
      "source": [
        "with open('test_loss.txt', 'w') as f:\n",
        "    f.write('\\n'.join([str(s) for s in test_loss_list]))\n",
        "\n",
        "with open('valid_loss.txt', 'w') as f:\n",
        "    f.write('\\n'.join([str(s) for s in valid_loss_list]))\n",
        "\n",
        "with open('valid_blue.txt', 'w') as f:\n",
        "    f.write('\\n'.join([str(s) for s in valid_bleu_list]))"
      ],
      "id": "afC6uEVGXIPL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greek-travel"
      },
      "source": [
        "from random import choice\n",
        "\n",
        "for _ in range(5):\n",
        "    idx = choice(range(len(valid_output)))\n",
        "\n",
        "    context = valid_input[idx]\n",
        "    encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
        "    x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
        "    y = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10)[0]\n",
        "\n",
        "    intent = len(encoded_input) + 1\n",
        "\n",
        "    predicted = y[intent:]\n",
        "    completion = tokenizer_output.decode(predicted, True)\n",
        "    print(f'Input:            {context}')\n",
        "    print(f'Predicted output: {completion}')\n",
        "    print(f'Real output:      {valid_output[idx]}')\n",
        "    print('--------------------------------------------------')"
      ],
      "id": "greek-travel",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF3lesDzM_UY"
      },
      "source": [
        "idx = choice(range(len(valid_output)))\n",
        "\n",
        "context = valid_input[idx]\n",
        "encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
        "x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
        "y, attention_state = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10, output_attention=True)\n",
        "\n",
        "intent = len(encoded_input) + 1\n",
        "\n",
        "predicted = y[0][intent:]\n",
        "completion = tokenizer_output.decode(predicted,)\n",
        "print(f'Input:            {context}')\n",
        "print(f'Predicted output: {completion}')\n",
        "print(f'Real output:      {valid_output[idx]}')\n",
        "print('--------------------------------------------------')\n"
      ],
      "id": "eF3lesDzM_UY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uY06xs7n5sx"
      },
      "source": [
        "fig, plots = plt.subplots(nrows=number_of_layers, ncols=number_of_heads, figsize=(30, 20))\n",
        "\n",
        "axis_text = tokenizer_input.decode(encoded_input, True).split()\n",
        "\n",
        "axis_text.append('<sep>')\n",
        "\n",
        "axis_text += tokenizer_input.decode(predicted, True).split()\n",
        "\n",
        "axis_text.append('<eos>')\n",
        "\n",
        "limit = len(axis_text)\n",
        "for bi in range(number_of_layers):\n",
        "    for hi in range(number_of_heads):\n",
        "        attetion_plot = torch.zeros(limit, limit)\n",
        "        for di in range(limit):\n",
        "            attetion_plot[:di, :di] = attention_state[bi][di][0,hi,:di,:di].data\n",
        "\n",
        "        ax = plots[bi][hi]\n",
        "        ax.matshow(attetion_plot.numpy(), cmap='bone')\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_xticklabels([''] + axis_text, rotation=90)\n",
        "        ax.set_yticklabels([''] + axis_text)\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "        # Set up a title\n",
        "        ax.set_title(f'Block {bi + 1} Head {hi + 1}', size=25, pad=30)\n",
        "        \n",
        "plt.show()"
      ],
      "id": "1uY06xs7n5sx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXFro4HhyE8D"
      },
      "source": [
        "# In case the previous cell is not plotting anything, uncomment the code below and execute. After that, the plotting should be fine.\n",
        "# %matplotlib inline\n",
        "# import numpy as np\n",
        "# x = np.linspace(0, 10, 100)\n",
        "\n",
        "# fig = plt.figure()\n",
        "# plt.plot(x, np.sin(x), '-')\n",
        "# plt.plot(x, np.cos(x), '--');"
      ],
      "id": "CXFro4HhyE8D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxvZ1nVstR7j"
      },
      "source": [
        "#Calculate BLEU"
      ],
      "id": "bxvZ1nVstR7j"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlVOSUDaNqaz"
      },
      "source": [
        "def clean_tokens(sentence):\n",
        "    return sentence.replace('@@ ', '').replace(' @', '').replace('@ ', '')"
      ],
      "id": "JlVOSUDaNqaz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VAAvPyR4GMv"
      },
      "source": [
        "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# smooth = SmoothingFunction().method7\n",
        "\n",
        "translation_results = []\n",
        "eval_results = []\n",
        "bleu_results = []\n",
        "for idx, context in enumerate(valid_input):\n",
        "    encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
        "    x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
        "    y = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10)[0]\n",
        "\n",
        "    intent = len(encoded_input) + 1\n",
        "    predicted = y[intent:]\n",
        "    completion = clean_tokens(tokenizer_output.decode(predicted, True))\n",
        "    translation_results.append(completion)\n",
        "\n",
        "    eval = clean_tokens(valid_output[idx])\n",
        "    eval_results.append(eval)\n",
        "    # bleu = sentence_bleu([eval], completion, smoothing_function=smooth)\n",
        "    # bleu_results.append(bleu)\n",
        "\n",
        "# print(f\"Averare BLEU: {np.mean(bleu_results)}\")"
      ],
      "id": "4VAAvPyR4GMv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xQ1sDwfWS9S"
      },
      "source": [
        "with open('valid.txt', 'w') as f:\n",
        "    f.write(\"\\n\".join(translation_results))\n",
        "\n",
        "with open('eval.txt', 'w') as f:\n",
        "    f.write(\"\\n\".join(eval_results))"
      ],
      "id": "4xQ1sDwfWS9S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8TILVBwWncc"
      },
      "source": [
        "!perl mosesdecoder/scripts/generic/multi-bleu.perl eval.txt < valid.txt"
      ],
      "id": "U8TILVBwWncc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0ccdKVEi_51"
      },
      "source": [
        "!cat valid.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > valid.detok.txt\n",
        "!cat eval.txt | mosesdecoder/scripts/tokenizer/detokenizer.perl -l lv > eval.detok.txt"
      ],
      "id": "L0ccdKVEi_51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ_G2YbxgdnH"
      },
      "source": [
        "!pip install sacrebleu"
      ],
      "id": "hZ_G2YbxgdnH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7Z2dQh7gnq2"
      },
      "source": [
        "import sacrebleu\n",
        "\n",
        "with open('eval.detok.txt', 'r') as f:\n",
        "    eval_results = [l.strip() for l in f.readlines()]\n",
        "with open('valid.detok.txt', 'r') as f:\n",
        "    translation_results = [l.strip() for l in f.readlines()]\n",
        "\n",
        "refs = [eval_results]\n",
        "sys = translation_results\n",
        "bleu = sacrebleu.corpus_bleu(sys, refs)\n",
        "print(bleu.score)"
      ],
      "id": "C7Z2dQh7gnq2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8b-0-iFkRRA"
      },
      "source": [
        "#Interactive translator"
      ],
      "id": "q8b-0-iFkRRA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHYXSA190x8G"
      },
      "source": [
        "context = input(\"Enter your English text to translate: \")\n",
        "\n",
        "# Predict Latvian output\n",
        "encoded_input = tokenizer_input.encode(tokenizer_input.tokenize(context, block_size))\n",
        "x = torch.tensor(encoded_input, dtype=torch.long)[None,...].to(trainer.device)\n",
        "y, attention_state = sample(model, x, block_size, temperature=1.0, sample=False, top_k=10, output_attention=True)\n",
        "\n",
        "intent = len(encoded_input) + 1\n",
        "\n",
        "predicted = y[0][intent:]\n",
        "completion = tokenizer_output.decode(predicted, True)\n",
        "print(f'Input:            {context}')\n",
        "print(f'Predicted output: {completion}')\n",
        "\n",
        "\n",
        "# Plot attention\n",
        "fig, plots = plt.subplots(nrows=number_of_layers, ncols=number_of_heads, figsize=(30, 20))\n",
        "\n",
        "axis_text = tokenizer_input.decode(encoded_input, True).split()\n",
        "\n",
        "axis_text.append('<sep>')\n",
        "\n",
        "axis_text += tokenizer_input.decode(predicted, True).split()\n",
        "\n",
        "axis_text.append('<eos>')\n",
        "\n",
        "limit = len(axis_text)\n",
        "for bi in range(number_of_layers):\n",
        "    for hi in range(number_of_heads):\n",
        "        attetion_plot = torch.zeros(limit, limit)\n",
        "        for di in range(limit):\n",
        "            attetion_plot[:di, :di] = attention_state[bi][di][0,hi,:di,:di].data\n",
        "\n",
        "        ax = plots[bi][hi]\n",
        "        ax.matshow(attetion_plot.numpy(), cmap='bone')\n",
        "\n",
        "        # Set up axes\n",
        "        ax.set_xticklabels([''] + axis_text, rotation=90)\n",
        "        ax.set_yticklabels([''] + axis_text)\n",
        "\n",
        "        # Show label at every tick\n",
        "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "        # Set up a title\n",
        "        ax.set_title(f'Block {bi + 1} Head {hi + 1}', size=25, pad=30)\n",
        "        \n",
        "plt.show()"
      ],
      "id": "LHYXSA190x8G",
      "execution_count": null,
      "outputs": []
    }
  ]
}